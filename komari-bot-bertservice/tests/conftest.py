"""Pytest é…ç½®å’Œå…±äº« fixtures"""

import sys
from pathlib import Path
from unittest.mock import MagicMock

import numpy as np
import pytest
from fastapi.testclient import TestClient

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.config import settings
from app.main import app
from app.services.inference_engine import ONNXInferenceEngine

# =============================================================================
# å…¨å±€ pytest é…ç½®
# =============================================================================

def pytest_configure(config):
    """Pytest åˆå§‹åŒ–é…ç½®"""
    config.addinivalue_line(
        "markers", "unit: Unit tests"
    )
    config.addinivalue_line(
        "markers", "integration: Integration tests"
    )
    config.addinivalue_line(
        "markers", "slow: Slow running tests"
    )


@pytest.fixture(scope="session")
def test_config():
    """æµ‹è¯•é…ç½®"""
    return settings


# =============================================================================
# Mock fixtures
# =============================================================================

@pytest.fixture
def mock_tokenizer():
    """Mock åˆ†è¯å™¨"""
    tokenizer = MagicMock()

    def mock_encode(text: str) -> dict:
        """æ¨¡æ‹Ÿç¼–ç ï¼Œè¿”å›å›ºå®šå½¢çŠ¶çš„æ•°ç»„"""
        # è¿”å›å½¢çŠ¶ (1, 128) çš„æ•°ç»„
        input_ids = np.random.randint(0, 1000, size=(1, 128), dtype=np.int64)
        attention_mask = np.ones((1, 128), dtype=np.int64)
        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
        }

    tokenizer.encode = mock_encode
    return tokenizer


@pytest.fixture
def mock_onnx_session():
    """Mock ONNX Runtime ä¼šè¯"""
    session = MagicMock()

    # æ¨¡æ‹Ÿæ¨ç†è¾“å‡ºï¼šè¿”å› logits
    # å½¢çŠ¶: (1, 3) - 1ä¸ªæ ·æœ¬ï¼Œ3ä¸ªç±»åˆ«
    mock_logits = np.array([[0.1, 0.7, 0.2]], dtype=np.float32)

    def mock_run(output_names, input_feed):
        """æ¨¡æ‹Ÿæ¨ç†è¿è¡Œ"""
        return [mock_logits]

    session.run = mock_run

    # æ¨¡æ‹Ÿè¾“å…¥è¾“å‡ºåç§°
    session.get_inputs.return_value = [MagicMock(name="input_ids"), MagicMock(name="attention_mask")]
    session.get_outputs.return_value = [MagicMock(name="output")]

    # æ¨¡æ‹Ÿ SessionOptions
    session.get_session_options.return_value = MagicMock()

    return session


@pytest.fixture
def mock_inference_engine(mock_tokenizer):
    """Mock æ¨ç†å¼•æ“"""
    engine = MagicMock(spec=ONNXInferenceEngine)

    # è®¾ç½®é»˜è®¤è¿”å›å€¼
    engine.score.return_value = (0.65, "normal", 0.92)
    engine.score_batch.return_value = [
        (0.65, "normal", 0.92),
        (0.15, "low_value", 0.88),
    ]
    engine._cache = {}
    engine.cache_size = 1024

    return engine


# =============================================================================
# FastAPI æµ‹è¯•å®¢æˆ·ç«¯
# =============================================================================

@pytest.fixture
def test_client():
    """FastAPI æµ‹è¯•å®¢æˆ·ç«¯"""
    return TestClient(app)


@pytest.fixture
def test_client_with_mock_engine(mock_inference_engine):
    """å¸¦æœ‰ mock æ¨ç†å¼•æ“çš„æµ‹è¯•å®¢æˆ·ç«¯"""
    # è®¾ç½® mock å¼•æ“åˆ° app state
    app.state.inference_engine = mock_inference_engine
    return TestClient(app)


# =============================================================================
# æµ‹è¯•æ•°æ® fixtures
# =============================================================================

@pytest.fixture
def sample_messages():
    """ç¤ºä¾‹æ¶ˆæ¯æ•°æ®"""
    return [
        {
            "message": "å“ˆå“ˆå“ˆ",
            "context": "",
            "expected_category": "low_value",
        },
        {
            "message": "ä»Šå¤©å¤©æ°”çœŸå¥½å•Š",
            "context": "æ˜¨å¤©ä¸‹é›¨äº†",
            "expected_category": "normal",
        },
        {
            "message": "æˆ‘éœ€è¦å¸®åŠ©è§£å†³è¿™ä¸ªé—®é¢˜",
            "context": "æœ‰äººçŸ¥é“æ€ä¹ˆå¤„ç†å—ï¼Ÿ",
            "expected_category": "interrupt",
        },
    ]


@pytest.fixture
def edge_case_messages():
    """è¾¹ç•Œæƒ…å†µæµ‹è¯•æ•°æ®"""
    return [
        {"message": "", "context": ""},  # ç©ºæ¶ˆæ¯
        {"message": "a" * 600, "context": ""},  # è¶…é•¿æ¶ˆæ¯
        {"message": "ç‰¹æ®Šå­—ç¬¦ï¼š!@#$%^&*()", "context": "æµ‹è¯•ä¸Šä¸‹æ–‡"},
        {"message": "Emoji æµ‹è¯• ğŸ˜‚ğŸ‰", "context": "è¡¨æƒ…åŒ…"},
    ]


@pytest.fixture
def batch_test_data():
    """æ‰¹é‡æµ‹è¯•æ•°æ®"""
    return {
        "messages": [
            {"message": "æµ‹è¯•æ¶ˆæ¯1", "context": "ä¸Šä¸‹æ–‡1"},
            {"message": "æµ‹è¯•æ¶ˆæ¯2", "context": "ä¸Šä¸‹æ–‡2"},
            {"message": "å“ˆå“ˆå“ˆ", "context": ""},
        ]
    }


# =============================================================================
# è·¯å¾„ fixtures
# =============================================================================

@pytest.fixture
def mock_model_path(tmp_path):
    """ä¸´æ—¶æ¨¡å‹è·¯å¾„"""
    model_dir = tmp_path / "models"
    model_dir.mkdir()
    model_file = model_dir / "test_model.onnx"
    model_file.write_text("mock model content")
    return str(model_file)


@pytest.fixture
def mock_tokenizer_path(tmp_path):
    """ä¸´æ—¶åˆ†è¯å™¨è·¯å¾„"""
    tokenizer_dir = tmp_path / "tokenizer"
    tokenizer_dir.mkdir()
    (tokenizer_dir / "config.json").write_text("{}")
    (tokenizer_dir / "vocab.txt").write_text("vocab")
    return str(tokenizer_dir)


# =============================================================================
# ç¯å¢ƒå˜é‡
# =============================================================================

@pytest.fixture(autouse=True)
def set_test_env_vars(monkeypatch):
    """è®¾ç½®æµ‹è¯•ç¯å¢ƒå˜é‡"""
    monkeypatch.setenv("MODEL_PATH", "/tmp/test_model.onnx")
    monkeypatch.setenv("TOKENIZER_PATH", "/tmp/test_tokenizer")
    monkeypatch.setenv("LOG_LEVEL", "WARNING")  # å‡å°‘æµ‹è¯•æ—¥å¿—å™ªéŸ³


# =============================================================================
# çœŸå®æ¨ç†å¼•æ“ fixturesï¼ˆç”¨äºé›†æˆæµ‹è¯•ï¼‰
# =============================================================================

@pytest.fixture(scope="session")
def real_engine():
    """çœŸå®çš„æ¨ç†å¼•æ“ï¼ˆç”¨äºé›†æˆæµ‹è¯•ï¼‰

    å¦‚æœæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡ä½¿ç”¨æ­¤ fixture çš„æµ‹è¯•ã€‚
    """
    model_path = "./models/bert_scoring.onnx"
    tokenizer_path = "./models/tokenizer"

    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    if not Path(model_path).exists() or not Path(tokenizer_path).exists():
        pytest.skip(f"Model files not found at {model_path} or {tokenizer_path}")

    try:
        engine = ONNXInferenceEngine(
            model_path=model_path,
            tokenizer_path=tokenizer_path,
            cache_size=10,  # å°ç¼“å­˜ç”¨äºæµ‹è¯•
        )
        return engine
    except Exception as e:
        pytest.skip(f"Failed to create real engine: {e}")


@pytest.fixture(scope="session")
def real_engine_with_small_cache():
    """å°ç¼“å­˜æ¨ç†å¼•æ“ï¼ˆç”¨äºæµ‹è¯•æ·˜æ±°é€»è¾‘ï¼‰

    å¦‚æœæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡ä½¿ç”¨æ­¤ fixture çš„æµ‹è¯•ã€‚
    """
    model_path = "./models/bert_scoring.onnx"
    tokenizer_path = "./models/tokenizer"

    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    if not Path(model_path).exists() or not Path(tokenizer_path).exists():
        pytest.skip(f"Model files not found at {model_path} or {tokenizer_path}")

    try:
        engine = ONNXInferenceEngine(
            model_path=model_path,
            tokenizer_path=tokenizer_path,
            cache_size=2,  # æå°ç¼“å­˜
        )
        return engine
    except Exception as e:
        pytest.skip(f"Failed to create real engine with small cache: {e}")
