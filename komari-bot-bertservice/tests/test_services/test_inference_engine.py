"""ONNX æ¨ç†å¼•æ“æµ‹è¯•"""

from unittest.mock import MagicMock, patch

import numpy as np
import pytest

from app.services.inference_engine import ONNXInferenceEngine

# =============================================================================
# åˆå§‹åŒ–æµ‹è¯•
# =============================================================================

class TestONNXInferenceEngineInit:
    """æ¨ç†å¼•æ“åˆå§‹åŒ–æµ‹è¯•"""

    def test_init_with_valid_paths(self, mock_model_path, mock_tokenizer_path):
        """æµ‹è¯•ï¼šæœ‰æ•ˆè·¯å¾„æ—¶çš„åˆå§‹åŒ–"""
        # æ³¨æ„ï¼šè¿™éœ€è¦çœŸå®çš„æ¨¡å‹æ–‡ä»¶ï¼Œæ‰€ä»¥è¿™é‡Œä¸»è¦æµ‹è¯•è·¯å¾„å¤„ç†é€»è¾‘
        # å®é™…æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬éœ€è¦æ¨¡æ‹Ÿ ONNX Runtime
        pass

    def test_init_sets_cache_size(self, mock_model_path, mock_tokenizer_path):
        """æµ‹è¯•ï¼šç¼“å­˜å¤§å°è®¾ç½®æ­£ç¡®"""
        with patch("app.services.inference_engine.TokenizerWrapper"):
            with patch("app.services.inference_engine.ort.InferenceSession"):
                engine = ONNXInferenceEngine(
                    mock_model_path,
                    mock_tokenizer_path,
                    cache_size=512,
                )
                assert engine.cache_size == 512

    def test_init_calculates_threads_correctly(self, mock_model_path, mock_tokenizer_path):
        """æµ‹è¯•ï¼šçº¿ç¨‹æ•°è®¡ç®—æ­£ç¡®"""
        with patch("app.services.inference_engine.TokenizerWrapper"):
            with patch("app.services.inference_engine.ort.InferenceSession"):
                with patch("os.cpu_count", return_value=4):
                    engine = ONNXInferenceEngine(
                        mock_model_path,
                        mock_tokenizer_path,
                    )
                    # åº”è¯¥æ˜¯ min(4, 8) = 4
                    assert engine.num_threads == 4

    def test_init_caps_threads_at_8(self, mock_model_path, mock_tokenizer_path):
        """æµ‹è¯•ï¼šçº¿ç¨‹æ•°æœ€å¤šä¸º 8"""
        with patch("app.services.inference_engine.TokenizerWrapper"):
            with patch("app.services.inference_engine.ort.InferenceSession"):
                with patch("os.cpu_count", return_value=16):
                    engine = ONNXInferenceEngine(
                        mock_model_path,
                        mock_tokenizer_path,
                    )
                    # åº”è¯¥æ˜¯ min(16, 8) = 8
                    assert engine.num_threads == 8


# =============================================================================
# å•æ¡è¯„åˆ†æµ‹è¯•
# =============================================================================

class TestSingleScoring:
    """å•æ¡è¯„åˆ†æµ‹è¯•"""

    def test_score_returns_tuple(self, mock_inference_engine):
        """æµ‹è¯•ï¼šè¿”å›å€¼æ˜¯å…ƒç»„"""
        result = mock_inference_engine.score("test", "context")
        assert isinstance(result, tuple)
        assert len(result) == 3

    def test_score_returns_correct_types(self, mock_inference_engine):
        """æµ‹è¯•ï¼šè¿”å›å€¼ç±»å‹æ­£ç¡®"""
        score, category, confidence = mock_inference_engine.score("test", "context")

        assert isinstance(score, float)
        assert isinstance(category, str)
        assert isinstance(confidence, float)

    def test_score_in_range(self, mock_inference_engine):
        """æµ‹è¯•ï¼šè¯„åˆ†åœ¨ 0.0-1.0 èŒƒå›´å†…"""
        score, _, _ = mock_inference_engine.score("test", "context")
        assert 0.0 <= score <= 1.0

    def test_score_category_valid(self, mock_inference_engine):
        """æµ‹è¯•ï¼šåˆ†ç±»æ ‡ç­¾æœ‰æ•ˆ"""
        _, category, _ = mock_inference_engine.score("test", "context")
        assert category in ["low_value", "normal", "interrupt"]

    def test_score_confidence_in_range(self, mock_inference_engine):
        """æµ‹è¯•ï¼šç½®ä¿¡åº¦åœ¨ 0.0-1.0 èŒƒå›´å†…"""
        _, _, confidence = mock_inference_engine.score("test", "context")
        assert 0.0 <= confidence <= 1.0

    def test_score_without_context(self, mock_inference_engine):
        """æµ‹è¯•ï¼šæ²¡æœ‰ä¸Šä¸‹æ–‡ä¹Ÿèƒ½è¯„åˆ†"""
        mock_inference_engine.score.return_value = (0.5, "normal", 0.9)
        score, category, confidence = mock_inference_engine.score("test", "")
        assert score == 0.5


# =============================================================================
# æ‰¹é‡è¯„åˆ†æµ‹è¯•
# =============================================================================

class TestBatchScoring:
    """æ‰¹é‡è¯„åˆ†æµ‹è¯•"""

    def test_score_batch_with_valid_input(self, mock_inference_engine):
        """æµ‹è¯•ï¼šæœ‰æ•ˆè¾“å…¥çš„æ‰¹é‡è¯„åˆ†"""
        items = [
            {"message": "test1", "context": "ctx1"},
            {"message": "test2", "context": "ctx2"},
        ]

        results = mock_inference_engine.score_batch(items)

        assert len(results) == 2
        assert all(isinstance(r, tuple) and len(r) == 3 for r in results)

    def test_score_batch_with_empty_list(self, mock_inference_engine):
        """æµ‹è¯•ï¼šç©ºåˆ—è¡¨è¿”å›ç©ºç»“æœ"""
        mock_inference_engine.score_batch.return_value = []
        results = mock_inference_engine.score_batch([])
        assert results == []

    def test_score_batch_with_single_item(self, mock_inference_engine):
        """æµ‹è¯•ï¼šå•é¡¹åˆ—è¡¨è°ƒç”¨å•æ¡è¯„åˆ†"""
        items = [{"message": "test", "context": "ctx"}]

        mock_inference_engine.score.return_value = (0.7, "normal", 0.85)
        results = mock_inference_engine.score_batch(items)

        assert len(results) == 1
        assert results[0] == (0.7, "normal", 0.85)

    def test_score_batch_preserves_order(self, mock_inference_engine):
        """æµ‹è¯•ï¼šæ‰¹é‡ç»“æœä¿æŒåŸå§‹é¡ºåº"""
        items = [
            {"message": "test1", "context": "ctx1"},
            {"message": "test2", "context": "ctx2"},
            {"message": "test3", "context": "ctx3"},
        ]

        mock_inference_engine.score_batch.return_value = [
            (0.1, "low_value", 0.8),
            (0.5, "normal", 0.9),
            (0.9, "interrupt", 0.95),
        ]

        results = mock_inference_engine.score_batch(items)

        # éªŒè¯é¡ºåº
        assert results[0][0] == 0.1
        assert results[1][0] == 0.5
        assert results[2][0] == 0.9


# =============================================================================
# ç¼“å­˜æµ‹è¯•
# =============================================================================

class TestCaching:
    """ç¼“å­˜åŠŸèƒ½æµ‹è¯•"""

    def test_cache_key_generation(self, mock_inference_engine):
        """æµ‹è¯•ï¼šç¼“å­˜é”®ç”Ÿæˆæ­£ç¡®"""
        # æµ‹è¯•ç¼“å­˜é”®çš„å”¯ä¸€æ€§
        key1 = mock_inference_engine._get_cache_key("message", "context")
        key2 = mock_inference_engine._get_cache_key("message", "context")
        key3 = mock_inference_engine._get_cache_key("message", "different")

        assert key1 == key2
        assert key1 != key3

    def test_cache_key_format(self, mock_inference_engine):
        """æµ‹è¯•ï¼šç¼“å­˜é”®æ ¼å¼æ­£ç¡®"""
        key = mock_inference_engine._get_cache_key("ä½ å¥½", "ä¸–ç•Œ")
        assert "ä¸–ç•Œ" in key
        assert "ä½ å¥½" in key

    def test_cache_add_increases_size(self, mock_inference_engine):
        """æµ‹è¯•ï¼šæ·»åŠ ç¼“å­˜å¢åŠ å¤§å°"""
        initial_size = len(mock_inference_engine._cache)
        mock_inference_engine._add_to_cache("key", (0.5, "normal", 0.9))
        assert len(mock_inference_engine._cache) == initial_size + 1

    def test_cache_eviction_when_full(self, mock_inference_engine):
        """æµ‹è¯•ï¼šç¼“å­˜æ»¡æ—¶æ‰§è¡Œ LRU é©±é€"""
        # è®¾ç½®å°ç¼“å­˜
        mock_inference_engine.cache_size = 3
        mock_inference_engine._cache = MagicMock()

        # æ·»åŠ  4 ä¸ªé¡¹ç›®
        for i in range(4):
            mock_inference_engine._add_to_cache(f"key{i}", (i * 0.1, "normal", 0.9))

        # åº”è¯¥åªæœ‰ 3 ä¸ªé¡¹ç›®ï¼ˆæœ€åä¸€ä¸ªè¢«é©±é€ï¼‰
        assert len(mock_inference_engine._cache) == 3


# =============================================================================
# è¾…åŠ©æ–¹æ³•æµ‹è¯•
# =============================================================================

class TestHelperMethods:
    """è¾…åŠ©æ–¹æ³•æµ‹è¯•"""

    def test_softmax_normalization(self):
        """æµ‹è¯•ï¼šSoftmax è¾“å‡ºå½’ä¸€åŒ–"""
        logits = np.array([1.0, 2.0, 3.0])
        probs = ONNXInferenceEngine._softmax(logits)

        # éªŒè¯å’Œä¸º 1
        assert np.isclose(probs.sum(), 1.0)

        # éªŒè¯æ‰€æœ‰å€¼ä¸ºæ­£
        assert np.all(probs > 0)

    def test_softmax_large_values(self):
        """æµ‹è¯•ï¼šSoftmax å¤„ç†å¤§å€¼"""
        logits = np.array([100.0, 200.0, 300.0])
        probs = ONNXInferenceEngine._softmax(logits)
        assert not np.any(np.isnan(probs))
        assert not np.any(np.isinf(probs))

    def test_class_to_score_range(self):
        """æµ‹è¯•ï¼šç±»åˆ«è½¬æ¢è¯„åˆ†åœ¨æ­£ç¡®èŒƒå›´"""
        probs = np.array([0.2, 0.6, 0.2])
        engine = ONNXInferenceEngine.__new__(ONNXInferenceEngine)
        score = engine._class_to_score(0, probs)

        # ä½¿ç”¨æƒé‡ [0.0, 0.55, 1.0]
        # score = 0*0.2 + 0.55*0.6 + 1.0*0.2 = 0.53
        assert 0.0 <= score <= 1.0

    @pytest.mark.parametrize("score,expected", [
        (0.1, "low_value"),
        (0.5, "normal"),
        (0.9, "interrupt"),
        (0.0, "low_value"),
        (0.3, "normal"),
        (0.8, "normal"),
    ])
    def test_score_to_category_mapping(self, score, expected):
        """æµ‹è¯•ï¼šè¯„åˆ†åˆ°åˆ†ç±»çš„æ˜ å°„æ­£ç¡®"""
        category = ONNXInferenceEngine._score_to_category(score)
        assert category == expected


# =============================================================================
# è¾¹ç•Œæƒ…å†µæµ‹è¯•
# =============================================================================

class TestEdgeCases:
    """è¾¹ç•Œæƒ…å†µæµ‹è¯•"""

    def test_empty_message(self, mock_inference_engine):
        """æµ‹è¯•ï¼šç©ºæ¶ˆæ¯å¤„ç†"""
        mock_inference_engine.score.return_value = (0.1, "low_value", 0.7)
        score, category, confidence = mock_inference_engine.score("", "")
        assert category == "low_value"

    def test_very_long_message(self, mock_inference_engine):
        """æµ‹è¯•ï¼šè¶…é•¿æ¶ˆæ¯å¤„ç†"""
        long_message = "a" * 600
        mock_inference_engine.score.return_value = (0.5, "normal", 0.8)
        # åº”è¯¥èƒ½å¤„ç†ï¼Œå¯èƒ½æˆªæ–­
        score, _, _ = mock_inference_engine.score(long_message, "context")
        assert isinstance(score, float)

    def test_unicode_message(self, mock_inference_engine):
        """æµ‹è¯•ï¼šUnicode æ¶ˆæ¯å¤„ç†"""
        unicode_message = "æµ‹è¯•ä¸­æ–‡å­—ç¬¦ ğŸ˜‚ğŸ‰"
        mock_inference_engine.score.return_value = (0.6, "normal", 0.85)
        score, _, _ = mock_inference_engine.score(unicode_message, "ä¸Šä¸‹æ–‡")
        assert isinstance(score, float)

    def test_special_characters(self, mock_inference_engine):
        """æµ‹è¯•ï¼šç‰¹æ®Šå­—ç¬¦å¤„ç†"""
        special_message = "!@#$%^&*()_+-=[]{}|;':\",./<>?"
        mock_inference_engine.score.return_value = (0.4, "normal", 0.8)
        score, _, _ = mock_inference_engine.score(special_message, "test")
        assert isinstance(score, float)
