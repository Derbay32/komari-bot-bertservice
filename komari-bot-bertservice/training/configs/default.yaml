# BERT 模型训练默认配置
#
# 使用方法:
#   python training/train.py --data-path ./data/train.json --config training/configs/default.yaml

# =============================================================================
# 模型配置
# =============================================================================
model:
  # 预训练模型名称（HuggingFace model hub）
  name: "hfl/chinese-bert-wwm-ext"

  # 分类数量：low_value=0, normal=1, interrupt=2
  num_labels: 3

  # 最大序列长度（会被截断或填充到这个长度）
  max_length: 128

  # Dropout 概率
  dropout: 0.1

# =============================================================================
# 训练配置
# =============================================================================
training:
  # 训练轮数
  epochs: 10

  # 批次大小（每个 GPU）
  batch_size: 32

  # 学习率
  learning_rate: 2.0e-5

  # Warmup 比例
  warmup_ratio: 0.1

  # 梯度累积步数（模拟更大批次）
  # batch_size * gradient_accumulation_steps = 有效批次大小
  gradient_accumulation_steps: 1

  # 混合精度训练（需要 GPU）
  fp16: false

# =============================================================================
# 优化器配置
# =============================================================================
optimizer:
  # 权重衰减（L2 正则化）
  weight_decay: 0.01

  # Adam epsilon
  adam_epsilon: 1.0e-8

# =============================================================================
# 学习率调度器配置
# =============================================================================
scheduler:
  # 调度器类型：linear, cosine, constant, constant_with_warmup
  type: "linear"

  # Warmup 步数（null 则由 warmup_ratio 自动计算）
  num_warmup_steps: null

# =============================================================================
# Early Stopping 配置
# =============================================================================
early_stopping:
  # 容忍轮数（验证集指标不提升的轮数）
  patience: 3

  # 最小改善阈值
  min_delta: 0.001

# =============================================================================
# 日志配置
# =============================================================================
logging:
  # 每个 N 步记录一次
  steps: 100

  # 每个 N 步评估一次
  eval_steps: 500

# =============================================================================
# 输出配置
# =============================================================================
output:
  # 保留的检查点数量（删除旧的）
  save_total_limit: 3

  # 保存策略：steps, epoch, no
  save_strategy: "steps"

  # 每个 N 步保存一次
  save_steps: 500

# =============================================================================
# 数据配置
# =============================================================================
data:
  # 验证集比例
  test_size: 0.1

  # 随机种子
  seed: 42

# =============================================================================
# 标签映射
# =============================================================================
labels:
  low_value: 0
  normal: 1
  interrupt: 2
